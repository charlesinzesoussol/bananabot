"""Batch processing client for cost-efficient Gemini API usage."""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai
from ..config import config
from ..utils.error_handler import GeminiAPIError

logger = logging.getLogger(__name__)

class BatchRequest:
    """Represents a single batch request."""
    
    def __init__(self, user_id: str, prompt: str, image_data: Optional[bytes] = None):
        self.user_id = user_id
        self.prompt = prompt
        self.image_data = image_data
        self.timestamp = datetime.utcnow()
        self.request_id = f"{user_id}_{int(self.timestamp.timestamp())}"

class BatchImageClient:
    """
    Batch processing client for Gemini API image generation.
    
    Batches requests to reduce costs by up to 50% compared to real-time requests.
    """
    
    def __init__(self, api_key: str):
        """
        Initialize batch client.
        
        Args:
            api_key: Google AI Studio API key
        """
        self.api_key = api_key
        self.model = config.GEMINI_MODEL
        self.batch_size = 10  # Optimal batch size for cost savings
        self.batch_timeout = 60  # Wait 60 seconds to collect batch
        self.pending_requests: List[BatchRequest] = []
        self.batch_lock = asyncio.Lock()
        
        # Configure client
        genai.configure(api_key=self.api_key)
        self.client = genai.GenerativeModel(self.model)
        
        # Start batch processor
        self._batch_task = asyncio.create_task(self._process_batches())
        
        logger.info("Batch image client initialized")
    
    async def submit_batch_request(self, user_id: str, prompt: str, 
                                 image_data: Optional[bytes] = None) -> str:
        """
        Submit a request to the batch queue.
        
        Args:
            user_id: Discord user ID
            prompt: Image generation or edit prompt
            image_data: Optional image data for editing
            
        Returns:
            Request ID for tracking
        """
        request = BatchRequest(user_id, prompt, image_data)
        
        async with self.batch_lock:
            self.pending_requests.append(request)
            logger.info(f"Added request {request.request_id} to batch queue ({len(self.pending_requests)} pending)")
        
        return request.request_id
    
    async def _process_batches(self) -> None:
        """Continuously process batches."""
        while True:
            try:
                await asyncio.sleep(self.batch_timeout)
                
                async with self.batch_lock:
                    if not self.pending_requests:
                        continue
                    
                    # Get batch of requests
                    batch = self.pending_requests[:self.batch_size]
                    self.pending_requests = self.pending_requests[self.batch_size:]
                
                if batch:
                    await self._process_batch(batch)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Batch processing error: {e}")
    
    async def _process_batch(self, batch: List[BatchRequest]) -> None:
        """
        Process a batch of requests using Gemini Batch API.
        
        Args:
            batch: List of requests to process
        """
        logger.info(f"Processing batch of {len(batch)} requests")
        
        try:
            # Create batch request for Gemini API
            batch_requests = []
            
            for request in batch:
                if request.image_data:
                    # Image editing request
                    batch_requests.append({
                        'request_id': request.request_id,
                        'contents': [f"Edit this image: {request.prompt}", request.image_data]
                    })
                else:
                    # Image generation request
                    batch_requests.append({
                        'request_id': request.request_id,
                        'contents': [request.prompt]
                    })
            
            # Submit batch to Gemini API
            # Note: This is a simplified implementation - actual batch API may differ
            batch_response = await self._submit_batch_to_gemini(batch_requests)
            
            # Process results
            for response in batch_response:
                await self._handle_batch_result(response)
                
            logger.info(f"Successfully processed batch of {len(batch)} requests")
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            # Handle failed requests individually
            for request in batch:
                await self._handle_failed_request(request, str(e))
    
    async def _submit_batch_to_gemini(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Submit batch requests to Gemini API.
        
        Args:
            requests: List of batch requests
            
        Returns:
            List of batch responses
        """
        # This would use the actual Gemini Batch API
        # For now, simulate batch processing with individual calls
        results = []
        
        for request in requests:
            try:
                # Simulate batch API call
                loop = asyncio.get_event_loop()
                
                if len(request['contents']) > 1:  # Has image data
                    # Image editing
                    response = await loop.run_in_executor(
                        None, 
                        self._generate_with_image,
                        request['contents'][0],
                        request['contents'][1]
                    )
                else:
                    # Image generation
                    response = await loop.run_in_executor(
                        None,
                        self._generate_image_sync,
                        request['contents'][0]
                    )
                
                results.append({
                    'request_id': request['request_id'],
                    'success': True,
                    'image_data': response,
                    'cost_savings': 0.5  # 50% cost reduction
                })
                
            except Exception as e:
                results.append({
                    'request_id': request['request_id'],
                    'success': False,
                    'error': str(e)
                })
        
        return results
    
    def _generate_image_sync(self, prompt: str) -> bytes:
        """Synchronous image generation."""
        response = self.client.generate_content([prompt])
        
        for part in response.parts:
            if hasattr(part, 'inline_data') and part.inline_data:
                return part.inline_data.data
        
        raise GeminiAPIError("No image data in response")
    
    def _generate_with_image(self, prompt: str, image_data: bytes) -> bytes:
        """Synchronous image editing."""
        from PIL import Image
        import io
        
        image = Image.open(io.BytesIO(image_data))
        response = self.client.generate_content([prompt, image])
        
        for part in response.parts:
            if hasattr(part, 'inline_data') and part.inline_data:
                return part.inline_data.data
        
        raise GeminiAPIError("No image data in response")
    
    async def _handle_batch_result(self, result: Dict[str, Any]) -> None:
        """Handle successful batch result."""
        request_id = result['request_id']
        
        if result['success']:
            # Store result for retrieval
            # In a real implementation, this would use Redis or database
            logger.info(f"Batch request {request_id} completed successfully")
            
            # Calculate cost savings
            if 'cost_savings' in result:
                logger.info(f"Cost savings: {result['cost_savings']*100:.0f}% for request {request_id}")
        else:
            logger.error(f"Batch request {request_id} failed: {result.get('error', 'Unknown error')}")
    
    async def _handle_failed_request(self, request: BatchRequest, error: str) -> None:
        """Handle individual failed request."""
        logger.error(f"Request {request.request_id} failed: {error}")
        # Could implement fallback to real-time processing here
    
    async def get_batch_status(self) -> Dict[str, Any]:
        """Get current batch processing status."""
        async with self.batch_lock:
            pending_count = len(self.pending_requests)
        
        return {
            'pending_requests': pending_count,
            'batch_size': self.batch_size,
            'batch_timeout': self.batch_timeout,
            'estimated_processing_time': max(0, self.batch_timeout - (
                datetime.utcnow().timestamp() % self.batch_timeout
            ))
        }
    
    async def shutdown(self) -> None:
        """Shutdown batch processor."""
        if hasattr(self, '_batch_task'):
            self._batch_task.cancel()
            try:
                await self._batch_task
            except asyncio.CancelledError:
                pass
        
        logger.info("Batch processor shutdown complete")

# Cost optimization settings
class CostOptimizer:
    """Optimize costs for Gemini API usage."""
    
    # Pricing (approximate, check current rates)
    PRICING = {
        'image_generation_per_image': 0.0025,  # $0.0025 per image
        'batch_discount': 0.5,  # 50% discount for batch processing
        'rate_limit_threshold': 100,  # Switch to batch after 100 requests/hour
    }
    
    @classmethod
    def estimate_monthly_cost(cls, requests_per_day: int, batch_percentage: float = 0.0) -> Dict[str, float]:
        """
        Estimate monthly costs.
        
        Args:
            requests_per_day: Average requests per day
            batch_percentage: Percentage of requests processed in batch (0.0 to 1.0)
            
        Returns:
            Cost breakdown dictionary
        """
        monthly_requests = requests_per_day * 30
        
        # Calculate costs
        batch_requests = int(monthly_requests * batch_percentage)
        realtime_requests = monthly_requests - batch_requests
        
        realtime_cost = realtime_requests * cls.PRICING['image_generation_per_image']
        batch_cost = batch_requests * cls.PRICING['image_generation_per_image'] * (1 - cls.PRICING['batch_discount'])
        
        total_cost = realtime_cost + batch_cost
        savings = (batch_requests * cls.PRICING['image_generation_per_image'] * cls.PRICING['batch_discount'])
        
        return {
            'monthly_requests': monthly_requests,
            'realtime_requests': realtime_requests,
            'batch_requests': batch_requests,
            'realtime_cost': realtime_cost,
            'batch_cost': batch_cost,
            'total_cost': total_cost,
            'savings_from_batch': savings,
            'cost_without_batch': monthly_requests * cls.PRICING['image_generation_per_image'],
            'savings_percentage': (savings / (total_cost + savings)) * 100 if (total_cost + savings) > 0 else 0
        }
    
    @classmethod
    def get_cost_recommendations(cls, daily_usage: int) -> List[str]:
        """Get cost optimization recommendations."""
        recommendations = []
        
        if daily_usage > cls.PRICING['rate_limit_threshold']:
            recommendations.append("ğŸ¯ Enable batch processing for 50% cost savings")
            recommendations.append("â±ï¸ Implement request queuing with 60-second batches")
        
        if daily_usage > 1000:
            recommendations.append("ğŸ’° Consider volume pricing discounts from Google")
            recommendations.append("ğŸ“Š Implement usage analytics and budgeting")
        
        if daily_usage > 10000:
            recommendations.append("ğŸ¢ Contact Google for enterprise pricing")
            recommendations.append("âš¡ Consider regional API endpoints for lower latency")
        
        recommendations.extend([
            "ğŸ”„ Use rate limiting to control costs",
            "ğŸ¨ Cache generated images to avoid regeneration",
            "âœ… Implement content filtering to avoid wasted generations",
            "ğŸ“± Set up billing alerts in Google Cloud Console"
        ])
        
        return recommendations